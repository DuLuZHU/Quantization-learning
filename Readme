LeNet-5原文：Y. LECUN, L. BOTTOU, Y. BENGIO, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE,1998,86(11):2278-2324.
            (https://ieeexplore.ieee.org/document/726791)
一、网络结构
LetNet-5是一个较简单的卷积神经网络。其结构：输入的二维图像（单通道），先经过两次卷积层到池化层，再经过全连接层，最后为输出层。
整体上是：input layer->convulational layer->pooling layer->activation function->convulational layer->pooling layer->activation function->convulational layer->fully connect layer->fully connect layer->output layer.
整个 LeNet-5 网络总共包括7层（不含输入层），分别是：C1、S2、C3、S4、C5、F6、OUTPUT。

几个参数：
层编号特点:英文字母+数字
英文字母代表以下一种：C→卷积层、S→下采样层（池化）、F→全连接层
数字代表当前是第几层，而非第几卷积层

术语解释：
参数→权重w与偏置b
连接数→连线数
参数计算：每个卷积核对应于一个偏置b，卷积核的大小对应于权重w的个数(特别注意通道数)

二、输入层（Input Layer）
输入层（INPUT）是 32x32 像素的图像，注意通道数为1。

三、C1 层
C1 层是卷积层，使用 6 个 5×5 大小的卷积核，padding=0，stride=1进行卷积，得到 6 个 28×28 大小的特征图：32-5+1=28。
参数个数：(5*5+1)*6=156，其中5*5为卷积核的25个参数w，1为偏置项b。
连接数：156*28*28=122304，其中156为单次卷积过程连线数，28*28为输出特征层，每一个像素都由前面卷积得到，即总共经历28*28次卷积。

四、S2 层
S2 层是下采样层，使用 6 个 2×2 大小的卷积核进行池化，padding=0，stride=2，得到 6 个 14×14 大小的特征图：28/2=14。
S2 层其实相当于下采样层+激活层。先是下采样，然后激活函数 sigmoid 非线性输出。先对 C1 层 2x2 的视野求和，然后进入激活函数。
参数个数：(1+1)*6=12，其中第一个 1 为池化对应的 2*2 感受野中最大的那个数的权重 w，第二个 1 为偏置 b。
连接数：(2*2+1)*6*14*14= 5880，虽然只选取 2*2 感受野之和，但也存在 2*2 的连接数，1 为偏置项的连接，14*14 为输出特征层，每一个像素都由前面卷积得到，即总共经历 14*14 次卷积。

五、C3 层
C3 层是卷积层，使用 16 个 5×5xn 大小的卷积核，padding=0，stride=1 进行卷积，得到 16 个 10×10 大小的特征图：14-5+1=10。
16 个卷积核并不是都与 S2 的 6 个通道层进行卷积操作，C3 的前六个特征图（0,1,2,3,4,5）由 S2 的相邻三个特征图作为输入，对应的卷积核尺寸为：5x5x3；接下来的 6 个特征图（6,7,8,9,10,11）由 S2 的相邻四个特征图作为输入对应的卷积核尺寸为：5x5x4；接下来的 3 个特征图（12,13,14）号特征图由 S2 间断的四个特征图作为输入对应的卷积核尺寸为：5x5x4；最后的 15 号特征图由 S2 全部(6 个)特征图作为输入，对应的卷积核尺寸为：5x5x6。
值得注意的是，卷积核是 5×5 且具有 3 个通道，每个通道各不相同，这也是下面计算时 5*5 后面还要乘以3,4,6的原因。这是多通道卷积的计算方法。
参数个数：(5*5*3+1)*6+(5*5*4+1)*6+(5*5*4+1)*3+(5*5*6+1)*1=1516。
连接数：1516*10*10 = 151600。10*10为输出特征层，每一个像素都由前面卷积得到，即总共经历10*10次卷积。

六、S4 层
S4 层与 S2 一样也是降采样层，使用 16 个 2×2 大小的卷积核进行池化，padding=0，stride=2，得到 16 个 5×5 大小的特征图：10/2=5。
参数个数：(1+1)*16=32。
连接数：(2*2+1)*16*5*5= 2000。

七、C5 层
C5 层是卷积层，使用 120 个 5×5x16 大小的卷积核，padding=0，stride=1进行卷积，得到 120 个 1×1 大小的特征图：5-5+1=1。即相当于 120 个神经元的全连接层。
值得注意的是，与C3层不同，这里120个卷积核都与S4的16个通道层进行卷积操作。
参数个数：(5*5*16+1)*120=48120。
连接数：48120*1*1=48120。

八、F6层
F6 是全连接层，共有 84 个神经元，与 C5 层进行全连接，即每个神经元都与 C5 层的 120 个特征图相连。计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过 sigmoid 函数输出。
F6 层有 84 个节点，对应于一个 7x12 的比特图，-1 表示白色，1 表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84=10164。
参数个数：(120+1)*84=10164。
连接数：(120+1)*84=10164。

九、OUTPUT层
最后的 Output 层也是全连接层，是 Gaussian Connections，采用了 RBF 函数（即径向欧式距离函数），计算输入向量和参数向量之间的欧式距离（目前已经被Softmax 取代）。
Output 层共有 10 个节点，分别代表数字 0 到 9。
参数个数：84*10=840。
连接数：84*10=840。

